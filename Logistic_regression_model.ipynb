{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_regression_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQYIZg7ilWRk1vp7D10+7d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shyDaniel/Binary_Classification/blob/master/Logistic_regression_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GoNvkkHr2ur",
        "colab_type": "text"
      },
      "source": [
        "## **BINARY CLASSIFICATION - PREDICTING WHETHER A PERSON'S SALARY WILL BE HIGHER THAN $ 50,000, INCLUDING TWO DIFFERENT APPROACHES: DISCRIMINATIVE MODEL - LOGISTIC REGRESSION, AND GENERATING MODEL - NAIVE BAYES**\n",
        "\n",
        "# **This notebook contains the logistic regression model.**\n",
        "\n",
        "Kaggle: https://www.kaggle.com/c/ml2020spring-hw2/overview\n",
        "\n",
        "Hanyu Song 03/23/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDxJ6ExboIer",
        "colab_type": "code",
        "outputId": "8ac7e19e-c794-4321-9be0-49e67392026a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "!gdown --id '1JuYMZyt0-jwVIwiVs-mgQE8i_O3QUyT3' --output Xtrain.zip\n",
        "!gdown --id '1iVZaPjgTmwwp683hV8cw-7Q2r3AunasO' --output Xtest.zip\n",
        "!gdown --id '1C0eGiL5azG49lnS45LxCdbrY0Ce4nNU2'\n",
        "!unzip Xtrain.zip\n",
        "!unzip Xtest.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JuYMZyt0-jwVIwiVs-mgQE8i_O3QUyT3\n",
            "To: /content/Xtrain.zip\n",
            "2.79MB [00:00, 88.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iVZaPjgTmwwp683hV8cw-7Q2r3AunasO\n",
            "To: /content/Xtest.zip\n",
            "100% 1.42M/1.42M [00:00<00:00, 93.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1C0eGiL5azG49lnS45LxCdbrY0Ce4nNU2\n",
            "To: /content/Y_train\n",
            "100% 423k/423k [00:00<00:00, 58.6MB/s]\n",
            "Archive:  Xtrain.zip\n",
            "  inflating: X_train                 \n",
            "Archive:  Xtest.zip\n",
            "  inflating: X_test                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp_Jll9Tutxm",
        "colab_type": "text"
      },
      "source": [
        "## **LOGISTIC REGRESSION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ8r_cLHfRl5",
        "colab_type": "text"
      },
      "source": [
        "**CONVERTING DATASETS INTO NUMPY MATRICES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2V-Dv24dFeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "X_train_fpath = '/content/X_train'\n",
        "Y_train_fpath = '/content/Y_train'\n",
        "X_test_fpath = '/content/X_test'\n",
        "output_fpath = '/output_{}.csv'\n",
        "\n",
        "# Parse csv files to numpy array\n",
        "with open(X_train_fpath) as f:\n",
        "    next(f)\n",
        "    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
        "with open(Y_train_fpath) as f:\n",
        "    next(f)\n",
        "    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\n",
        "with open(X_test_fpath) as f:\n",
        "    next(f)\n",
        "    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUQo3yZMfbN3",
        "colab_type": "text"
      },
      "source": [
        "**NORMALIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZRgN5ftfavp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c7a2a7ed-99d7-4862-cb3e-f4b36083aa50"
      },
      "source": [
        "def _normalize(X, train = True, specified_column = None, X_mean = None, X_std = None):\n",
        "    # This function normalizes specific columns of X.\n",
        "    # The mean and standard variance of training data will be reused when processing testing data.\n",
        "    #\n",
        "    # Arguments:\n",
        "    #     X: data to be processed\n",
        "    #     train: 'True' when processing training data, 'False' for testing data\n",
        "    #     specific_column: indexes of the columns that will be normalized. If 'None', all columns\n",
        "    #         will be normalized.\n",
        "    #     X_mean: mean value of training data, used when train = 'False'\n",
        "    #     X_std: standard deviation of training data, used when train = 'False'\n",
        "    # Outputs:\n",
        "    #     X: normalized data\n",
        "    #     X_mean: computed mean value of training data\n",
        "    #     X_std: computed standard deviation of training data\n",
        "\n",
        "    if specified_column == None:\n",
        "        specified_column = np.arange(X.shape[1])\n",
        "    if train:\n",
        "        X_mean = np.mean(X[:, specified_column] ,0).reshape(1, -1)\n",
        "        X_std  = np.std(X[:, specified_column], 0).reshape(1, -1)\n",
        "\n",
        "    X[:,specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8)\n",
        "     \n",
        "    return X, X_mean, X_std\n",
        "\n",
        "def _train_dev_split(X, Y, dev_ratio = 0.25):\n",
        "    # This function spilts data into training set and development set.\n",
        "    train_size = int(len(X) * (1 - dev_ratio))\n",
        "    return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]\n",
        "\n",
        "# Normalize training and testing data\n",
        "X_train, X_mean, X_std = _normalize(X_train, train = True)\n",
        "X_test, _, _= _normalize(X_test, train = False, specified_column = None, X_mean = X_mean, X_std = X_std)\n",
        "    \n",
        "# Split data into training set and development set\n",
        "dev_ratio = 0.1\n",
        "X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio = dev_ratio)\n",
        "\n",
        "train_size = X_train.shape[0]\n",
        "dev_size = X_dev.shape[0]\n",
        "test_size = X_test.shape[0]\n",
        "data_dim = X_train.shape[1]\n",
        "print('Size of training set: {}'.format(train_size))\n",
        "print('Size of development set: {}'.format(dev_size))\n",
        "print('Size of testing set: {}'.format(test_size))\n",
        "print('Dimension of data: {}'.format(data_dim))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: 48830\n",
            "Size of development set: 5426\n",
            "Size of testing set: 27622\n",
            "Dimension of data: 510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbpp92IBlw5U",
        "colab_type": "text"
      },
      "source": [
        "**Some useful functions in Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rNZx6FXlwfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _shuffle(X, Y):\n",
        "    # This function shuffles two equal-length list/array, X and Y, together.\n",
        "    randomize = np.arange(len(X))\n",
        "    np.random.shuffle(randomize)\n",
        "    return (X[randomize], Y[randomize])\n",
        "\n",
        "def _sigmoid(z):\n",
        "    # Sigmoid function can be used to calculate probability.\n",
        "    # To avoid overflow, minimum/maximum output value is set.\n",
        "    return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))\n",
        "\n",
        "def _f(X, w, b):\n",
        "    # This is the logistic regression function, parameterized by w and b\n",
        "    #\n",
        "    # Arguements:\n",
        "    #     X: input data, shape = [batch_size, data_dimension]\n",
        "    #     w: weight vector, shape = [data_dimension, ]\n",
        "    #     b: bias, scalar\n",
        "    # Output:\n",
        "    #     predicted probability of each row of X being positively labeled, shape = [batch_size, ]\n",
        "    return _sigmoid(np.dot(X, w) + b)\n",
        "\n",
        "def _predict(X, w, b):\n",
        "    # This function returns a truth value prediction for each row of X \n",
        "    # by rounding the result of logistic regression function.\n",
        "    return np.round(_f(X, w, b)).astype(np.int)\n",
        "    \n",
        "def _accuracy(Y_pred, Y_label):\n",
        "    # This function calculates prediction accuracy\n",
        "    acc = 1 - np.mean(np.abs(Y_pred - Y_label))\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sz3VMQ1jJI1",
        "colab_type": "text"
      },
      "source": [
        "**CROSS-ENTROPY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSY9mOK8e3vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _cross_entropy_loss(y_pred, Y_label):\n",
        "    # This function computes the cross entropy.\n",
        "    #\n",
        "    # Arguements:\n",
        "    #     y_pred: probabilistic predictions, float vector\n",
        "    #     Y_label: ground truth labels, bool vector\n",
        "    # Output:\n",
        "    #     cross entropy, scalar\n",
        "    cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred))\n",
        "    return cross_entropy\n",
        "\n",
        "def _gradient(X, Y_label, w, b):\n",
        "    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n",
        "    y_pred = _f(X, w, b)\n",
        "    pred_error = Y_label - y_pred\n",
        "    w_grad = -np.sum(pred_error * X.T, 1) # ELEMENT-WISE PRODUCTION\n",
        "    b_grad = -np.sum(pred_error)\n",
        "    return w_grad, b_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx2JVEkxkMFC",
        "colab_type": "text"
      },
      "source": [
        "**TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mfLFQDcjGrw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5375caef-4f6d-4e11-90ca-320b1d145fd6"
      },
      "source": [
        "# Zero initialization for weights ans bias\n",
        "w = np.zeros((data_dim,)) \n",
        "b = np.zeros((1,))\n",
        "\n",
        "# Some parameters for training    \n",
        "max_iter = 50\n",
        "batch_size = 8\n",
        "learning_rate = 0.05\n",
        "\n",
        "# Keep the loss and accuracy at every iteration for plotting\n",
        "train_loss = []\n",
        "dev_loss = []\n",
        "train_acc = []\n",
        "dev_acc = []\n",
        "\n",
        "# Use adagrad to optimize learning rate\n",
        "adagradw = np.zeros((data_dim,))\n",
        "adagradb = 0\n",
        "eps = 0.0000000001\n",
        "\n",
        "# Iterative training\n",
        "for epoch in range(max_iter):\n",
        "    # Random shuffle at the begging of each epoch\n",
        "    X_train, Y_train = _shuffle(X_train, Y_train)\n",
        "        \n",
        "    # Mini-batch training\n",
        "    for idx in range(int(np.floor(train_size / batch_size))):\n",
        "        X = X_train[idx*batch_size:(idx+1)*batch_size]\n",
        "        Y = Y_train[idx*batch_size:(idx+1)*batch_size]\n",
        "\n",
        "        # Compute the gradient\n",
        "        w_grad, b_grad = _gradient(X, Y, w, b)\n",
        "        \n",
        "        adagradw += w_grad ** 2\n",
        "        adagradb += b_grad ** 2\n",
        "\n",
        "        # gradient descent update\n",
        "        # learning rate decay with time\n",
        "        w = w - learning_rate/np.sqrt(adagradw + eps) * w_grad\n",
        "        b = b - learning_rate/np.sqrt(adagradb + eps) * b_grad\n",
        "\n",
        "            \n",
        "    # Compute loss and accuracy of training set and development set\n",
        "    y_train_pred = _f(X_train, w, b)\n",
        "    Y_train_pred = np.round(y_train_pred)\n",
        "    train_acc.append(_accuracy(Y_train_pred, Y_train))\n",
        "    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size)\n",
        "\n",
        "    y_dev_pred = _f(X_dev, w, b)\n",
        "    Y_dev_pred = np.round(y_dev_pred)\n",
        "    dev_acc.append(_accuracy(Y_dev_pred, Y_dev))\n",
        "    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) / dev_size)\n",
        "\n",
        "print('Training loss: {}'.format(train_loss[-1]))\n",
        "print('Development loss: {}'.format(dev_loss[-1]))\n",
        "print('Training accuracy: {}'.format(train_acc[-1]))\n",
        "print('Development accuracy: {}'.format(dev_acc[-1]))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0.2737360233266404\n",
            "Development loss: 0.2939921198473981\n",
            "Training accuracy: 0.8852754454228957\n",
            "Development accuracy: 0.8772576483597494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDToVYNIBfFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "41114507-5dcb-4e38-c3d2-eb3799ec67da"
      },
      "source": [
        "# Zero initialization for weights ans bias\n",
        "w = np.zeros((data_dim,)) \n",
        "b = np.zeros((1,))\n",
        "\n",
        "# Some parameters for training    \n",
        "max_iter = 1000\n",
        "learning_rate = 1\n",
        "\n",
        "# Keep the loss and accuracy at every iteration for plotting\n",
        "train_loss = []\n",
        "dev_loss = []\n",
        "train_acc = []\n",
        "dev_acc = []\n",
        "\n",
        "# Use adagrad to optimize learning rate\n",
        "adagradw = np.zeros((data_dim,))\n",
        "adagradb = 0\n",
        "eps = 0.0000000001\n",
        "\n",
        "# Iterative training\n",
        "for epoch in range(max_iter):\n",
        "    # Random shuffle at the begging of each epoch\n",
        "    X_train, Y_train = _shuffle(X_train, Y_train)\n",
        "        \n",
        "    X = X_train\n",
        "    Y = Y_train\n",
        "\n",
        "        # Compute the gradient\n",
        "    w_grad, b_grad = _gradient(X, Y, w, b)\n",
        "        \n",
        "    adagradw += w_grad ** 2\n",
        "    adagradb += b_grad ** 2\n",
        "\n",
        "        # gradient descent update\n",
        "        # learning rate decay with time\n",
        "    w = w - learning_rate/np.sqrt(adagradw + eps) * w_grad\n",
        "    b = b - learning_rate/np.sqrt(adagradb + eps) * b_grad\n",
        "\n",
        "            \n",
        "    # Compute loss and accuracy of training set and development set\n",
        "    y_train_pred = _f(X_train, w, b)\n",
        "    Y_train_pred = np.round(y_train_pred)\n",
        "    train_acc.append(_accuracy(Y_train_pred, Y_train))\n",
        "    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size)\n",
        "\n",
        "    curloss = _cross_entropy_loss(y_train_pred, Y_train) / train_size\n",
        "\n",
        "    if (epoch % 100 == 0):\n",
        "      print (\"In the \",epoch,\" th iteration, the loss is: \", curloss)\n",
        "\n",
        "    y_dev_pred = _f(X_dev, w, b)\n",
        "    Y_dev_pred = np.round(y_dev_pred)\n",
        "    dev_acc.append(_accuracy(Y_dev_pred, Y_dev))\n",
        "    dev_loss.append(curloss)\n",
        "\n",
        "print('Training loss: {}'.format(train_loss[-1]))\n",
        "print('Development loss: {}'.format(dev_loss[-1]))\n",
        "print('Training accuracy: {}'.format(train_acc[-1]))\n",
        "print('Development accuracy: {}'.format(dev_acc[-1]))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In the  0  th iteration, the loss is:  5.640612352487127\n",
            "In the  100  th iteration, the loss is:  0.3658910358408261\n",
            "In the  200  th iteration, the loss is:  0.3213275246090302\n",
            "In the  300  th iteration, the loss is:  0.2959598198196976\n",
            "In the  400  th iteration, the loss is:  0.2851108377467621\n",
            "In the  500  th iteration, the loss is:  0.275907399324037\n",
            "In the  600  th iteration, the loss is:  0.2720910981554639\n",
            "In the  700  th iteration, the loss is:  0.2684718872802823\n",
            "In the  800  th iteration, the loss is:  0.26749078355455524\n",
            "In the  900  th iteration, the loss is:  0.26686323048524224\n",
            "Training loss: 0.265771404397878\n",
            "Development loss: 0.265771404397878\n",
            "Training accuracy: 0.885398320704485\n",
            "Development accuracy: 0.8761518614080354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWlp9Q6j2ylL",
        "colab_type": "text"
      },
      "source": [
        "**PLOT LOSS CURVE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbnk8bgl2xku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "527bf46e-ea14-4514-ebe9-75a80a6eba84"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loss curve\n",
        "plt.plot(train_loss)\n",
        "plt.plot(dev_loss)\n",
        "plt.title('Loss')\n",
        "plt.legend(['train', 'dev'])\n",
        "plt.savefig('loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Accuracy curve\n",
        "plt.plot(train_acc)\n",
        "plt.plot(dev_acc)\n",
        "plt.title('Accuracy')\n",
        "plt.legend(['train', 'dev'])\n",
        "plt.savefig('acc.png')\n",
        "plt.show()\n",
        "\n",
        "np.save('model_weights.npy', w)\n",
        "np.save('constant.npy', b)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEICAYAAAB25L6yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnT4J4XDXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCTiFPQiIhGnoBcRibj/DzOD7D3vHAznAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l26aVSboD11s",
        "colab_type": "text"
      },
      "source": [
        "## **Predicting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSQftxKFD1pE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict testing labels\n",
        "weights = np.load('model_weights.npy')\n",
        "cons = np.load('constant.npy')\n",
        "predictions = _predict(X_test, weights, cons)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JBKsqLfEYkZ",
        "colab_type": "text"
      },
      "source": [
        "## **Save result to a csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs3P4Y_KEYbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(np.empty((predictions.shape[0], 2)), index = np.arange(predictions.shape[0]) + 1, columns = ['id', 'label'], dtype = int)\n",
        "for i in range (predictions.shape[0]):\n",
        "  df.iloc[i, 0] = i\n",
        "  df.iloc[i, 1] = predictions[i]\n",
        "df.to_csv('submission.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}