{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Pn6YwkNtRFkQmFsit_TWXZU7M4bN5H-V",
      "authorship_tag": "ABX9TyMYU9ND+v6b2B4wICAN6Mh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shyDaniel/Binary_Classification/blob/master/DEC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSa-u8aCB-z4",
        "colab_type": "text"
      },
      "source": [
        "# DEC Implemented in Pytorch\n",
        "\n",
        "08/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke742ZK_uvrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# os.environ['PATH'] = '../caffe/build/tools:'+os.environ['PATH']\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import torchvision.datasets as dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADF5ANx4WDm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def acc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate clustering accuracy. Implements the Hungarian Algorithm\n",
        "    with linear_sum_assignment\n",
        "    \"\"\"\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    row, col = linear_sum_assignment(w.max() - w)\n",
        "    sum = 0\n",
        "    for i in range(len(row)):\n",
        "        sum += w[row[i], col[i]]\n",
        "    return sum * 1.0 / y_pred.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNTS9Szv-I-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "batch = 100\n",
        "path = '../mnist'\n",
        "train_set = dataset.MNIST(root=path, train=True, transform=transform, download=True)\n",
        "test_set = dataset.MNIST(root=path, train=False, transform=transform, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=batch,\n",
        "    shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_set,\n",
        "    batch_size=batch,\n",
        "    shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lil0lvvSIX_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DEC_AE(nn.Module):\n",
        "    \"\"\"\n",
        "    Stacked Auto Encoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, num_features):\n",
        "        super(DEC_AE, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.fc1 = nn.Linear(28 * 28, 500)\n",
        "        self.fc2 = nn.Linear(500, 500)\n",
        "        self.fc3 = nn.Linear(500, 2000)\n",
        "        self.fc4 = nn.Linear(2000, num_features)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc_d1 = nn.Linear(500, 28 * 28)\n",
        "        self.fc_d2 = nn.Linear(500, 500)\n",
        "        self.fc_d3 = nn.Linear(2000, 500)\n",
        "        self.fc_d4 = nn.Linear(num_features, 2000)\n",
        "        self.alpha = 1.0\n",
        "        self.clusterCenter = nn.Parameter(torch.zeros(num_classes, num_features))\n",
        "        self.pretrainMode = True\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def setPretrain(self, mode):\n",
        "        \"\"\"To set training mode to pretrain or not,\n",
        "        so that it can control to run only the Encoder or Encoder+Decoder\"\"\"\n",
        "        self.pretrainMode = mode\n",
        "\n",
        "    def updateClusterCenter(self, cc):\n",
        "        \"\"\"\n",
        "        To update the cluster center. This is a method for pre-train phase.\n",
        "        When a center is being provided by kmeans, we need to update it so\n",
        "        that it is available for further training\n",
        "        :param cc: the cluster centers to update, size of num_classes x num_features\n",
        "        \"\"\"\n",
        "        self.clusterCenter.data = torch.from_numpy(cc)\n",
        "\n",
        "    def getTDistribution(self, x, clusterCenter):\n",
        "        \"\"\"\n",
        "        student t-distribution, as same as used in t-SNE algorithm.\n",
        "         q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
        "\n",
        "         :param x: input data, in this context it is encoder output\n",
        "         :param clusterCenter: the cluster center from kmeans\n",
        "         \"\"\"\n",
        "        xe = torch.unsqueeze(x,1) - clusterCenter\n",
        "        q = 1.0 / (1.0 + (torch.sum(torch.mul(xe,xe), 2) / self.alpha))\n",
        "        q = q ** (self.alpha + 1.0) / 2.0\n",
        "        q = (q.t() / torch.sum(q, 1)).t()\n",
        "        return q\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1 * 28 * 28)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x_e = x\n",
        "        # if not in pretrain mode, we need encoder and t distribution output\n",
        "        if not self.pretrainMode:\n",
        "            return x_e, self.getTDistribution(x, self.clusterCenter)\n",
        "        ##### encoder is done, followed by decoder #####\n",
        "        x = self.fc_d4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc_d3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc_d2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc_d1(x)\n",
        "        x_de = x.view(-1, 1, 28, 28)\n",
        "        return x_e, x_de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFYUWXCOJC6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DEC:\n",
        "    def __init__(self, n_clusters, n_latent, alpha=1.0):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.n_latent = n_latent\n",
        "        self.alpha = alpha\n",
        "\n",
        "    @staticmethod\n",
        "    def target_distribution(q):\n",
        "        weight = (q ** 2) / q.sum(0)\n",
        "        # print('q',q)\n",
        "        return Variable((weight.t() / weight.sum(1)).t().data, requires_grad=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def kld(q, p):\n",
        "        return torch.sum(p * torch.log(p / q), dim=-1)\n",
        "\n",
        "    def validateOnCompleteTestData(self,test_loader,model):\n",
        "        model.eval()\n",
        "        to_eval = np.array([model(d[0])[0].data.cpu().numpy() for i,d in enumerate(test_loader)])\n",
        "        true_labels = np.array([d[1].cpu().numpy() for i,d in enumerate(test_loader)])\n",
        "        to_eval = np.reshape(to_eval,(to_eval.shape[0]*to_eval.shape[1],to_eval.shape[2]))\n",
        "        true_labels = np.reshape(true_labels,true_labels.shape[0]*true_labels.shape[1])\n",
        "        km = KMeans(n_clusters=len(np.unique(true_labels)), n_init=20, n_jobs=4)\n",
        "        y_pred = km.fit_predict(to_eval)\n",
        "        currentAcc = acc(true_labels, y_pred)\n",
        "        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
        "                      % (currentAcc, nmi(true_labels, y_pred)))\n",
        "        return currentAcc\n",
        "\n",
        "    def pretrain(self, train_loader, test_loader, epochs):\n",
        "        dec_ae = DEC_AE(self.n_clusters, self.n_latent)  # auto encoder\n",
        "        optimizer = optim.SGD(dec_ae.parameters(), lr=1, momentum=0.9)\n",
        "        best_acc = 0.0\n",
        "        for epoch in range(epochs):\n",
        "            dec_ae.train()\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(train_loader):\n",
        "                x = Variable(data[0])\n",
        "                optimizer.zero_grad()\n",
        "                x_ae, x_de = dec_ae(x)\n",
        "                loss = nn.functional.mse_loss(x_de, x, reduction = 'mean')\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.data.cpu().numpy()\n",
        "                if i % 100 == 99:  # print every 100 mini-batches\n",
        "                    print('[%d, %5d] loss: %.7f' %\n",
        "                          (epoch + 1, i + 1, running_loss / 100))\n",
        "                    running_loss = 0.0\n",
        "            # now we evaluate the accuracy with AE\n",
        "            dec_ae.eval()\n",
        "            currentAcc = self.validateOnCompleteTestData(test_loader, dec_ae)\n",
        "            if currentAcc > best_acc:\n",
        "                torch.save(dec_ae, 'bestModel'.format(best_acc))\n",
        "                best_acc = currentAcc\n",
        "\n",
        "    def clustering(self, mbk, x, model):\n",
        "        model.eval()\n",
        "        y_pred_ae, _ = model(x)\n",
        "        y_pred_ae = y_pred_ae.data.cpu().numpy()\n",
        "        mbk.partial_fit(y_pred_ae)\n",
        "        self.cluster_centers = mbk.cluster_centers_  # keep the cluster centers\n",
        "        model.updateClusterCenter(self.cluster_centers)\n",
        "\n",
        "    def train(self, train_loader, test_loader, epochs):\n",
        "        \"\"\"This method will start training for DEC cluster\"\"\"\n",
        "        model = torch.load(\"bestModel\")\n",
        "        model.setPretrain(False)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "        print('Initializing cluster center with pre-trained weights')\n",
        "        mbk = MiniBatchKMeans(n_clusters=self.n_clusters, n_init=20, batch_size=100)\n",
        "        got_cluster_center = False\n",
        "        for epoch in range(epochs):\n",
        "            for i, data in enumerate(train_loader):\n",
        "                x, label = data\n",
        "                x = Variable(x)\n",
        "                optimizer.zero_grad()\n",
        "                # step 1 - get cluster center from batch\n",
        "                # here we are using minibatch kmeans to be able to cope with larger dataset.\n",
        "                if not got_cluster_center:\n",
        "                    self.clustering(mbk, x, model)\n",
        "                    if epoch > 1:\n",
        "                        got_cluster_center = True\n",
        "                else:\n",
        "                    model.train()\n",
        "                    # now we start training with acquired cluster center\n",
        "                    feature_pred, q = model(x)\n",
        "                    p = self.target_distribution(q)\n",
        "                    loss = self.kld(q, p).mean()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "            currentAcc = self.validateOnCompleteTestData(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3-ULQJHmw47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec = DEC(10, 10)\n",
        "dec.pretrain(train_loader, test_loader, 100)\n",
        "dec.train(train_loader, test_loader, 200)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}